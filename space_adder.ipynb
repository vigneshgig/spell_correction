{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "space_adder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "GIa5BSSeY4Dw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OugBpPrqbhf-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# write all code in one cell\n",
        "\n",
        "# ========================Load data=========================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.models import Model\n",
        "import pickle\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/retrain_dataset_to_remove_space_no_blog','rb') as f:#  new_number_and_words_classification new_seqtoseq_classification\n",
        "    train_texts,train_output = pickle.load(f)\n",
        "\n",
        "\n",
        "\n",
        "# convert string to lower case\n",
        "# train_texts = train_df[1].values\n",
        "# for  m,i in enumerate(train_texts):\n",
        "#     if i == ' ' or i == '' or i == '  ':\n",
        "#       del train_texts[m]\n",
        "#       del train_output[m]\n",
        "# for  m,i in enumerate(train_texts_validation):\n",
        "#     if i == ' ' or i == '' or i == '  ':\n",
        "#       del train_texts_validation[m]\n",
        "#       del train_output_validation[m]\n",
        "\n",
        "      # train_texts = [s.lower() for s in train_texts]\n",
        "\n",
        "# # test_texts = test_df[1].values\n",
        "# train_texts_validation = [s[0].lower() for s in train_texts_validation]\n",
        "\n",
        "# =======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "# tkw =Tokenizer(num_words=None, oov_token='UNK')\n",
        "# tkw.fit_on_texts(train_output)\n",
        "# If we already have a character list, then replace the tk.word_index\n",
        "# If not, just skip below part\n",
        "\n",
        "# -----------------------Skip part start--------------------------\n",
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789 \"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "# tkw.word_index[' '] = len(tkw.word_index) + 2\n",
        "# -----------------------Skip part end----------------------------\n",
        "\n",
        "# Convert string to index\n",
        "train_sequences = tk.texts_to_sequences(train_texts)\n",
        "train_sequences_output = tk.texts_to_sequences(train_output)\n",
        "\n",
        "# train_sequences_validation = tk.texts_to_sequences(train_texts_validation)\n",
        "#Padding\n",
        "train_data = pad_sequences(train_sequences, maxlen=60, padding='post')\n",
        "train_data_output = pad_sequences(train_sequences_output, maxlen=60, padding='post')\n",
        "\n",
        "# train_data_validation = pad_sequences(train_sequences_validation, maxlen=90, padding='post')\n",
        "# for  k,i in enumerate(train_output):\n",
        "#   print(i)\n",
        "# train_output = pad_sequences(train_output,maxlen=87,padding='post')\n",
        "#   train_output[k] = data\n",
        "# for  k,i in enumerate(train_output_validation):\n",
        "# train_output_validation = pad_sequences(train_output_validation,maxlen=90,padding='post')\n",
        "#   train_output_validation[k] = data\n",
        "\n",
        "# test_data = pad_sequences(test_texts, maxlen=50, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "train_data = np.array(train_data, dtype='float32')\n",
        "train_classes = np.array(train_data_output, dtype='float32')\n",
        "# train_output_3 = np.array(train_output_3, dtype='float32')\n",
        "# train_data_validation = np.array(train_data_validation, dtype='float32')\n",
        "# train_output_validation = np.array(train_output_validation, dtype='float32')\n",
        "\n",
        "# =======================Get classes================\n",
        "# train_classes = train_df[0].values\n",
        "# train_class_list = [x - 1 for x in train_output]\n",
        "\n",
        "# test_classes = test_df[0].values\n",
        "# test_class_list = [x - 1 for x in test_classes]\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_classes = to_categorical(train_data_output)\n",
        "# train_classes_1 = to_categorical(train_output_3)\n",
        "# train_classes_validation = to_categorical(train_output_validation)\n",
        "# test_classes = to_categorical(test_class_list)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X-T67A04akQR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# =====================Char CNN=======================\n",
        "# parameter\n",
        "from keras.layers import Concatenate,Add,Bidirectional,LSTM,TimeDistributed,GRU,BatchNormalization,CuDNNLSTM\n",
        "from keras.utils import plot_model\n",
        "\n",
        "input_size = 60\n",
        "vocab_size = len(tk.word_index)\n",
        "embedding_size = 69\n",
        "conv_layers = [[256, 10],\n",
        "               [256, 10],\n",
        "               [256, 7],\n",
        "               [256, 7],\n",
        "               [256, 7],\n",
        "               [256, 3],\n",
        "               [256, 3],]\n",
        "fully_connected_layers = [200, 200]\n",
        "# num_of_classes = 70\n",
        "dropout_p = 0.5\n",
        "optimizer = 'Nadam'\n",
        "loss = 'categorical_crossentropy'\n",
        "\n",
        "# Embedding weights\n",
        "embedding_weights = []  # (70, 69)\n",
        "embedding_weights.append(np.zeros(vocab_size))  # (0, 69)\n",
        "\n",
        "for char, i in tk.word_index.items():  # from index 1 to 69\n",
        "    onehot = np.zeros(vocab_size)\n",
        "    onehot[i - 1] = 1\n",
        "    embedding_weights.append(onehot)\n",
        "\n",
        "embedding_weights = np.array(embedding_weights)\n",
        "print('Load')\n",
        "\n",
        "# Embedding layer Initialization\n",
        "embedding_layer = Embedding(vocab_size + 1,\n",
        "                            300,\n",
        "                            input_length=input_size,\n",
        "                            mask_zero=False)\n",
        "#                             weights=[embedding_weights])\n",
        "\n",
        "# Model Construction\n",
        "# Input\n",
        "inputs = Input(shape=(input_size,), name='input', dtype='int64')  # shape=(?, 1014)\n",
        "# Embedding\n",
        "x = embedding_layer(inputs)\n",
        "# x = Position_Embedding()(x)\n",
        "# x = Attention(8, 10)([x, x, x])\n",
        "\n",
        "# x1 = Flatten()(x)\n",
        "# predictions_1 = Dense(num_of_classes,activation='softmax')(x1)\n",
        "\n",
        "# from keras_pos_embd import PositionEmbedding\n",
        "# position_embedding = PositionEmbedding(\n",
        "#     input_shape=(None,),\n",
        "#     input_dim=90,     # The maximum absolute value of positions.\n",
        "#     output_dim=embedding_size,     # The dimension of embeddings.  # The index that presents padding (because `0` will be used in relative positioning).\n",
        "#     name='Pos-Embd',\n",
        "# )\n",
        "# xp = position_embedding(inputs)\n",
        "# Conv\n",
        "# for filter_num, filter_size in conv_layers:\n",
        "#     if filter_size == 10:\n",
        "# #       if k == 0:\n",
        "#         x_10 = Conv1D(filter_num, filter_size)(x)\n",
        "#         x_10 = Activation('relu')(x_10)\n",
        "# #         k = 1\n",
        "# #       else:\n",
        "# #         x_10 = Conv1D(filter_num, filter_size)(x_10)\n",
        "# #         x_10 = Activation('relu')(x_10)\n",
        "#     elif filter_size == 7:\n",
        "# #       if d == 0:\n",
        "#         x_7 = Conv1D(filter_num, filter_size)(x)\n",
        "#         x_7 = Activation('relu')(x_7)\n",
        "# #         d = 1\n",
        "# #       else:\n",
        "# #         x_7 = Conv1D(filter_num, filter_size)(x_7)\n",
        "# #         x_7 = Activation('relu')(x_7)\n",
        "#     elif filter_size == 5:\n",
        "# #       if v == 0:\n",
        "#         x_5 = Conv1D(filter_num, filter_size)(x)\n",
        "#         x_5 = Activation('relu')(x_5)\n",
        "# #         v = 1\n",
        "# #       else:\n",
        "# #         x_5 = Conv1D(filter_num, filter_size)(x_5)\n",
        "# #         x_5 = Activation('relu')(x_5)\n",
        "#     elif filter_size == 3:\n",
        "# #       if b == 0:\n",
        "#         x_3 = Conv1D(filter_num, filter_size)(x)\n",
        "#         x_3 = Activation('relu')(x_3)\n",
        "# #         b = 1\n",
        "# #       else:\n",
        "# #         x_3 = Conv1D(filter_num, filter_size)(x)\n",
        "# #         x_3 = Activation('relu')(x_3)\n",
        "        \n",
        "# x = Add()([x_10,x_7,x_5,x_3])\n",
        "# conv_layers = [[256,10],[256,10],[256,7],[256,7],[256,7],[256,3],[256,3]]\n",
        "# for filter_num, filter_size in conv_layers:\n",
        "# #     if filter_size == 10:\n",
        "#       x = Conv1D(filter_num, filter_size,padding='same')(x)\n",
        "#       x = Activation('elu')(x)\n",
        "\n",
        "# for filter_num, filter_size in conv_layers:\n",
        "#     if filter_size == 10:\n",
        "conv = Conv1D(100, 60,padding='same')(x)\n",
        "conv = BatchNormalization()(conv)\n",
        "conv = Activation('elu')(conv)\n",
        "conv = Dropout(0.3)(conv)\n",
        "# conv = MaxPooling1D(2,padding='valid')(conv)\n",
        "conv = Conv1D(50, 60,padding='same')(conv)\n",
        "conv = BatchNormalization()(conv)\n",
        "conv = Activation('elu')(conv)\n",
        "conv = Dropout(0.3)(conv)\n",
        "# conv = MaxPooling1D(2,padding='valid')(conv)\n",
        "# conv = Add()([conv,x])\n",
        "# conv = Conv1D(256, 70,padding='same')(conv)\n",
        "# conv = BatchNormalization()(conv)\n",
        "# conv = Activation('elu')(conv)\n",
        "# conv = Dropout(0.3)(conv)\n",
        "# # conv = Add()([conv,x])\n",
        "# # conv = MaxPooling1D(2,strides=3,padding='valid')(conv)\n",
        "# conv = Conv1D(100, 70,padding='same')(conv)\n",
        "# conv = Activation('elu')(conv)\n",
        "# conv = Conv1D(100, 90,padding='same')(conv)\n",
        "# conv = Activation('elu')(conv)\n",
        "# bilstm = Bidirectional(CuDNNLSTM(3,return_sequences=True,))(x)\n",
        "# bilstm_flatten = Flatten()(x)  # (None, 8704)\n",
        "# conv_flatten = Flatten()(conv)  # (None, 8704)\n",
        "\n",
        "x = Concatenate()([conv,x])\n",
        "#     if pooling_size != -1:\n",
        "#         x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)\n",
        "# conv_flatten = Flatten()(conv)  # (None, 8704)\n",
        "# \n",
        "x = Bidirectional(CuDNNLSTM(5,return_sequences=True))(x)\n",
        "# encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder\n",
        "# state_h = Add()([forward_h, backward_h])\n",
        "# state_c = Add()([forward_c, backward_c])\n",
        "# encoder_last = [state_h, state_c]\n",
        "\n",
        "\n",
        "\n",
        "#     if pooling_size != -1:\n",
        "#         x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)\n",
        "# bilstm_flatten = Flatten()(encoder)  # (None, 8704)\n",
        "# \n",
        "# x = Concatenate()([conv_flatten,bilstm_flatten])\n",
        "\n",
        "# x.shape\n",
        "# x = LSTM(1,)(x)\n",
        "# # Fully connected layers\n",
        "# for dense_size in fully_connected_layers:\n",
        "# x_2 = Dense(50,activation='elu')(x)\n",
        "# x = Flatten()(conv)\n",
        "# x = TimeDistributed(Dense(100, activation='elu'))(x)  # dense_size == 1024\n",
        "# x = TimeDistributed(Dropout(dropout_p))(x)\n",
        "# x = TimeDistributed(Dense(100, activation='relu'))(x)  # dense_size == 1024\n",
        "# x = TimeDistributed(Dropout(dropout_p))(x)\n",
        "\n",
        "# # Output Layer\n",
        "# d = len(tkw.word_index)\n",
        "predictions = TimeDistributed(Dense(38, activation='softmax'))(x)#\n",
        "\n",
        "# print(predictions.shape)\n",
        "# # Build model\n",
        "model = Model(inputs=inputs, outputs=[predictions])#\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['categorical_accuracy'])  # Adam, categorical_crossentropy\n",
        "model.summary()\n",
        "\n",
        "# # # 1000 training samples and 100 testing samples\n",
        "# # indices = np.arange(train_data.shape[0])\n",
        "# # np.random.shuffle(indices)\n",
        "# #\n",
        "# x_train = train_data[indices][:1000]\n",
        "# y_train = train_classes[indices][:1000]\n",
        "#\n",
        "# # x_test = test_data[:100]\n",
        "# # y_test = test_classes[:100]\n",
        "\n",
        "indices = np.arange(train_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "x_train = train_data[indices]\n",
        "y_train = train_classes[indices]\n",
        "# y_train_1 = train_classes_1[indices]\n",
        "# plot_model(model, to_file='/content/drive/My Drive/singewordvisualmodel.png')\n",
        "\n",
        "# Training\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TF9ph0XVaoVy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x=x_train, y=[y_train],#predictions_1\n",
        "          validation_split = 0.01,\n",
        "#           validation_data=(x_valid,y_valid),\n",
        "          batch_size=100,\n",
        "          epochs=10,shuffle=True,)#callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "biGPRRPoa4zf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O90y6poLaugJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datas = model.predict(train_data,verbose=1) # test = model.predict(data(['hidufitler']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bQs5A9u-a7_j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#check the predicted data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h82ee7K9a1FU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "whole = []\n",
        "for  i in datas:\n",
        "  lists = []\n",
        "  for  j in i:\n",
        "    lists.append(np.argmax(j))\n",
        "  whole.append(lists)\n",
        "\n",
        "reverse_dict = {}\n",
        "for  i in char_dict.items():\n",
        "  reverse_dict[i[1]] = i[0]\n",
        "\n",
        "whole_words =[]\n",
        "for  k in whole:\n",
        "  lists = []\n",
        "  for j in k:\n",
        "    if j:\n",
        "      lists.append(reverse_dict[j])\n",
        "#   print(lists)\n",
        "  whole_words.append(''.join(lists))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xNI8zfYVbFFl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in zip(train_texts[120000:],train_output[120000:],whole_words[120000:]):\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DTamTGSSb9-p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    model_json = model.to_json()\n",
        "    with open(\"/content/drive/My Drive/classification_words_new_space_model_3.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "except:\n",
        "    pass\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"/content/drive/My Drive/classification_words_new_space_model_3.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
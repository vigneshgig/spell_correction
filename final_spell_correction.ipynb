{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final_spell_correction.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"IQ0HWl8nF3RC","colab_type":"code","outputId":"6951e5d9-8bb0-4cfb-b5ff-cbffed4eee78","executionInfo":{"status":"ok","timestamp":1552539412042,"user_tz":-330,"elapsed":44819,"user":{"displayName":"Vignesh amudha","photoUrl":"https://lh4.googleusercontent.com/-uAm63ANriRI/AAAAAAAAAAI/AAAAAAAAAF8/s-20PVizA8U/s64/photo.jpg","userId":"10425760557979851046"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"azyCoL22Rmnm","colab_type":"code","outputId":"6de79080-5b20-4f98-c86c-27ef49a3fa58","executionInfo":{"status":"ok","timestamp":1552539444653,"user_tz":-330,"elapsed":1880,"user":{"displayName":"Vignesh amudha","photoUrl":"https://lh4.googleusercontent.com/-uAm63ANriRI/AAAAAAAAAAI/AAAAAAAAAF8/s-20PVizA8U/s64/photo.jpg","userId":"10425760557979851046"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n","\n","# construct a new vocabulary\n","alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789 \"\n","char_dict = {}\n","for i, char in enumerate(alphabet):\n","    char_dict[char] = i + 1\n","print(char_dict)\n","# Use char_dict to replace the tk.word_index\n","tk.word_index = char_dict.copy()\n","# print(tk.word_index)\n","# Add 'UNK' to the vocabulary\n","tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n","# print(tk.word_index)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '0': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36, ' ': 37}\n"],"name":"stdout"}]},{"metadata":{"id":"YgioQxwOR2YK","colab_type":"code","colab":{}},"cell_type":"code","source":["import pickle\n","with open('/content/drive/My Drive/final_spell_correction/word_index','wb') as f:\n","  pickle.dump(tk.word_index,f)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-q8SrOjPGNus","colab_type":"code","outputId":"013e0af6-e196-4f21-8d52-d9fe0610407f","executionInfo":{"status":"ok","timestamp":1552539451604,"user_tz":-330,"elapsed":3831,"user":{"displayName":"Vignesh amudha","photoUrl":"https://lh4.googleusercontent.com/-uAm63ANriRI/AAAAAAAAAAI/AAAAAAAAAF8/s-20PVizA8U/s64/photo.jpg","userId":"10425760557979851046"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"cell_type":"code","source":["from keras.models import model_from_json\n","# load json and create model\n","json_file = open('/content/drive/My Drive/final_spell_correction/classification_words_new_space_model_noblog_cnn_bilstm_no_cunnlstm5.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","model_space = model_from_json(loaded_model_json)\n","\n","# load weights into new model\n","model_space.load_weights(\"/content/drive/My Drive/final_spell_correction/classification_words_new_space_model_noblog_cnn_bilstm_no_cunnlstm5.h5\")\n","print(\"Loaded model from disk\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","Loaded model from disk\n"],"name":"stdout"}]},{"metadata":{"id":"MI3bps64OIOh","colab_type":"code","outputId":"6958c972-8531-42ca-e0cf-e1cc977bd776","executionInfo":{"status":"ok","timestamp":1552539453709,"user_tz":-330,"elapsed":3652,"user":{"displayName":"Vignesh amudha","photoUrl":"https://lh4.googleusercontent.com/-uAm63ANriRI/AAAAAAAAAAI/AAAAAAAAAF8/s-20PVizA8U/s64/photo.jpg","userId":"10425760557979851046"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from keras.models import model_from_json\n","# load json and create model\n","json_file = open('/content/drive/My Drive/final_spell_correction/cnn_bilstm_two_word_classification.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","model_two_word_spell_correction = model_from_json(loaded_model_json)\n","\n","# load weights into new model\n","model_two_word_spell_correction.load_weights(\"/content/drive/My Drive/final_spell_correction/cnn_bilstm_two_word_classification.h5\")\n","print(\"Loaded model from disk\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Loaded model from disk\n"],"name":"stdout"}]},{"metadata":{"id":"eEAPi0slon8K","colab_type":"code","colab":{}},"cell_type":"code","source":["# c,p = model_two_word_spell_correction.predict(np.array([1,3,5,3,4,5,34,5,6,44,5,6,4]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ChJxC_i8QHLZ","colab_type":"code","colab":{}},"cell_type":"code","source":["import spacy\n","stop_words =['ours', 'keep', 'in', 'enough', 'anything', 'latterly'\n"," , 'thereupon', 'your', 'if', 'as', 'each', 'his', 'but'\n"," , 'everywhere', 'hereupon', 'being', 'becoming', 'and',\n"," 'anyhow', 'serious', 'something', 'latter', \n"," 'namely', 'name', 'seemed', 'yourselves', 'toward', 'must', \n"," 'same', 'then', 'become', 'while', 'becomes', 'ourselves', 'perhaps', \n"," 'or', 'more', 'whose', 'along', 'own', 'thence', 'had', 'itself', \n"," 'top', 'whether', 'beside', 'into', 'on', 'per', 'whole', 'one', \n"," 'towards', 'himself', 'against', 'beyond', 'off', 'done', 'are', \n"," 'you', 'he', 'yours', 'an', 'myself', 'themselves', \n"," 'hereafter', 'else', 'have', 'neither', 'again', 'afterwards', \n"," 'under', 'its', 'due', 'always', 'be', 'over', 'therefore', \n"," 'very', 'at', 'during', 'nobody', 'where', \n"," 'whoever', 'across', 'thereafter', 'i', 'thereby', 'empty', \n"," 'move', 'put', 'through', 'since', 'my', 'wherein', 'became', 'thus',\n"," 'none', 'cannot', 'did', 'next', 'above', 'regarding', \n"," 'to', 'too', 'within', 'just', 'nothing', 'now', 'am', 'part', 'seems', 'than', 'alone', 'after', 'once', \n"," 'doing', 'otherwise', 'who', 'indeed', 'full', 'whence',\n"," 'before', 'how', 'although', 'mostly', 'take', 'between', 'these',\n"," 'whereas', 'former', 'whom', 'many', 'amongst', 'other',\n"," 'ca', 'besides', 'go', 'much', 'may', 'nowhere', 'together', \n"," 'him', 'her', 'there', 'say', 'throughout',\n"," 'whereby', 'mine', 'formerly', 'only', 'really', 'herein', \n"," 'show', 'might', 'hers', 'often', 'when', \n"," 'whereupon', 'those', 'rather', 'somewhere', 'give', 'here', \n"," 'do', 'used', 'does', 'me', 'seem', 'unless', 'sometime', \n"," 'almost', 'via', 'back', 'hereby', 'few', 'all', 'up', \n"," 'using', 'should', 'well', 'see', 'been', 'various', 'yourself', \n"," 'bottom', 'onto', 'side', 'for', 'everyone', 'will', \n"," 'several', 'however', 'meanwhile', 'can', 'everything', 'around', \n"," 'she', 'of', 'their', 'were', 'get', 'until', 'that', \n"," 'yet', 'already', 'both', 'by', 'somehow', 'any', 'please', \n"," 'whereafter', 'behind', 'therein', 'the', 'they', 'whenever', \n"," 'out', 'still', 'our', 'most', 'least', 'though', 'with', 'a', 'could',\n"," 'such', 'less', 'was', 'nor', 'others', 'why', 'about', 'never', 'so',\n"," 'us', 'wherever', 'beforehand', 'moreover', 'last', 'among', 'elsewhere',\n"," 'nevertheless', 'quite', 'upon', 'ever', 'anywhere', 'we', 'down', 'what', \n"," 'amount', 'whither', 'it', 'below', 'someone', 'either', 'is', 'some',\n"," 'even', 'also', 'from', 'except', 'further', 'herself', 'make', 'which',\n"," 'this', 'call', 'without', 'made', 're', 'sometimes', 'another', 'whatever',\n"," 'anyone', 'would', 'every', 'thru', 'them', 'anyway', 'hence', 'has', \n"," 'because', 'seeming',\"what's\",\"whats\",'-PRON-']\n","nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k0GKxBwojUjn","colab_type":"code","outputId":"5db0ad04-36e1-4dd3-b68b-0d0a4d879110","executionInfo":{"status":"ok","timestamp":1552539465215,"user_tz":-330,"elapsed":4906,"user":{"displayName":"Vignesh amudha","photoUrl":"https://lh4.googleusercontent.com/-uAm63ANriRI/AAAAAAAAAAI/AAAAAAAAAF8/s-20PVizA8U/s64/photo.jpg","userId":"10425760557979851046"}},"colab":{"base_uri":"https://localhost:8080/","height":513}},"cell_type":"code","source":["# from keras.utils import to_categorical\n","from numpy import argmax\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import LabelEncoder\n","import pickle\n","with open('/content/drive/My Drive/two_word_spell_correction_noblog_dataset','rb') as f:\n","    train_texts,train_output = pickle.load(f)\n","train_output_1 = []\n","for k,i in enumerate(train_output):\n","    train_output_1.append(i[1])\n","    del train_output[k][1]\n","\n","label_encoder = LabelEncoder()\n","integer_encoded = label_encoder.fit_transform(train_output)\n","print(integer_encoded)\n","# binary encode\n","onehot_encoder = OneHotEncoder(sparse=False)\n","integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n","print(onehot_encoded)\n","# invert first example\n","inverted = label_encoder.inverse_transform([argmax(onehot_encoded[-1, :])])\n","print(inverted)\n","\n","label_encoder_1 = LabelEncoder()\n","integer_encoded_1 = label_encoder_1.fit_transform(train_output_1)\n","print(integer_encoded_1)\n","# binary encode\n","onehot_encoder_1 = OneHotEncoder(sparse=False)\n","integer_encoded_1 = integer_encoded_1.reshape(len(integer_encoded_1), 1)\n","onehot_encoded_1 = onehot_encoder_1.fit_transform(integer_encoded_1)\n","print(onehot_encoded_1)\n","# invert first example\n","inverted_1 = label_encoder_1.inverse_transform([argmax(onehot_encoded_1[-1, :])])\n","print(inverted_1)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"},{"output_type":"stream","text":["[62  0 40 ... 16 68 39]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n","If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n","In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n","  warnings.warn(msg, FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[[0. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 1. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","['look']\n","[ 0 32  0 ... 57  0 55]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n","If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n","In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n","  warnings.warn(msg, FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[[1. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","['print']\n"],"name":"stdout"}]},{"metadata":{"id":"aWhHoFeSWgXN","colab_type":"code","colab":{}},"cell_type":"code","source":["import pickle\n","with open('/content/drive/My Drive/final_spell_correction/label_encoder','rb') as f:\n","#   pickle.dump([label_encoder,label_encoder_1],f)\n","  label_encoder,label_encoder_1 = pickle.load(f)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2_tchVJaTqzV","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"mycUlfF4OYio","colab_type":"code","colab":{}},"cell_type":"code","source":["import re\n","from keras.preprocessing.sequence import pad_sequences\n","def spell_correction(x):\n","  train_texts = [re.sub(r'[^\\x00-\\x7f]','',re.sub('[\\t\\r\\n,)([\\]!%|!#$%&*+,.-/:;<=>?@^_`{|}~?\\']','',str(i))).strip().lower() for i in x]\n","  train_texts_split = [i.split() for i in train_texts]\n","  training_data = []\n","  for i in train_texts_split:\n","      for  j in i:\n","          if len(j) == 1  or len(j) == 2:\n","              pass\n","          else:\n","              try:\n","                  int(j)\n","              except:\n","                  training_data.append(j)\n","  tokensd = [tokend.lower() for tokend in training_data if not tokend.lower() in stop_words]\n","  doc = nlp(str(' '.join(tokensd)))\n","  training_data = [str(lemm.lemma_) for lemm in doc]\n","  train_sequences = tk.texts_to_sequences(training_data)\n","  train_data = pad_sequences(train_sequences, maxlen=60, padding='post')\n","  train_data = np.array(train_data, dtype='float32')\n","  splitted_words = model_space.predict(train_data)\n","  changed_integer = []\n","  for  i in splitted_words:\n","    changed_integer.append(np.argmax(i,axis=1))\n","#   print(changed_integer)\n","  spell_words_token = []\n","  temp = []\n","  for j in changed_integer:\n","    for i in j: \n","      if i != 37 and i!= 0:\n","        temp.append(i)\n","      else:\n","        if temp != []:\n","          spell_words_token.append(temp)\n","          temp = []\n","  print(spell_words_token)\n","  for i,check_val in enumerate(spell_words_token):\n","    if len(check_val) <= 2:\n","      del spell_words_token[i]\n","  print(spell_words_token)\n","  train_data_spell = pad_sequences(spell_words_token, maxlen=13, padding='post')\n","#     train_data_spell = pad_sequences(spell, maxlen=13, padding='post')\n","\n","  train_data_spell = np.array(train_data_spell, dtype='float32')\n","  corrected_word_tokenised,corrected_word_tokenised_1  = model_two_word_spell_correction.predict(train_data_spell)\n","  inverted = label_encoder.inverse_transform(argmax(corrected_word_tokenised,axis=1))\n","  inverted_1 = label_encoder_1.inverse_transform(argmax(corrected_word_tokenised_1,axis=1))\n","#   print(inverted,inverted_1)\n","  corrected_words = ''\n","  for i in zip(inverted,inverted_1):\n","    if i[1] == ' ':\n","      corrected_words += i[0]+i[1]\n","    else:\n","      string  = i[0]+' '+i[1]+' '\n","      corrected_words += string \n","  print(corrected_words)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5rhEbnOglk9V","colab_type":"code","outputId":"cea7c39b-3644-4522-9407-d191c5e2ecf1","executionInfo":{"status":"ok","timestamp":1552540238665,"user_tz":-330,"elapsed":970,"user":{"displayName":"Vignesh amudha","photoUrl":"https://lh4.googleusercontent.com/-uAm63ANriRI/AAAAAAAAAAI/AAAAAAAAAF8/s-20PVizA8U/s64/photo.jpg","userId":"10425760557979851046"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["\n","import time\n","start = time.time()\n","spell_correction([\"\"])\n","end = time.time()\n","print(end - start)\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[[8, 9, 4, 21, 23, 23, 5, 4, 4, 4]]\n","[[8, 9, 4, 21, 23, 23, 5, 4, 4, 4]]\n","hindu wedding \n","0.06045651435852051\n"],"name":"stdout"}]},{"metadata":{"id":"cM6uQkaolrUm","colab_type":"code","colab":{}},"cell_type":"code","source":["  a = np.array([2,4,5,3,5,37,4346,35,37,37,0,0,0,0,0])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xNumJg_wcvQ4","colab_type":"code","outputId":"cb1d7591-66c5-4c08-83d4-5df0d76ee41a","executionInfo":{"status":"ok","timestamp":1551766397117,"user_tz":-330,"elapsed":788,"user":{"displayName":"Vignesh amudha","photoUrl":"https://lh4.googleusercontent.com/-uAm63ANriRI/AAAAAAAAAAI/AAAAAAAAAF8/s-20PVizA8U/s64/photo.jpg","userId":"10425760557979851046"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["np.where(a==37)[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([5, 8, 9])"]},"metadata":{"tags":[]},"execution_count":39}]},{"metadata":{"id":"W1raVeJSbpC8","colab_type":"code","outputId":"cc1e3701-8a13-4b42-a4a1-78b4d38157bb","executionInfo":{"status":"ok","timestamp":1551766322598,"user_tz":-330,"elapsed":822,"user":{"displayName":"Vignesh amudha","photoUrl":"https://lh4.googleusercontent.com/-uAm63ANriRI/AAAAAAAAAAI/AAAAAAAAAF8/s-20PVizA8U/s64/photo.jpg","userId":"10425760557979851046"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["[x[x!=37] for x in np.split(a, np.where(a==37)[0]) if len(x[(x!=0) & (x!=37)]) ]\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([2, 4, 5, 3, 5]), array([4346,   35])]"]},"metadata":{"tags":[]},"execution_count":36}]},{"metadata":{"id":"HyTf2hDidhRv","colab_type":"code","outputId":"04463ca0-afd8-42f0-acd9-ec0c5c51abe3","executionInfo":{"status":"ok","timestamp":1551766366932,"user_tz":-330,"elapsed":828,"user":{"displayName":"Vignesh amudha","photoUrl":"https://lh4.googleusercontent.com/-uAm63ANriRI/AAAAAAAAAAI/AAAAAAAAAF8/s-20PVizA8U/s64/photo.jpg","userId":"10425760557979851046"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"cell_type":"code","source":["for x in np.split(a, np.where(a==37)[0]):\n","  print(x)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[2 4 5 3 5]\n","[  37 4346   35]\n","[37]\n","[37  0  0  0  0  0]\n"],"name":"stdout"}]},{"metadata":{"id":"yoM0Yh4iecvq","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}
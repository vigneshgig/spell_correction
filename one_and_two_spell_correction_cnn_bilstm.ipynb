{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "one_and_two_spell_correction_cnn_bilstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "gMLYDXCZfkuU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVN7Q7vVgIbF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# write all code in one cell\n",
        "\n",
        "# ========================Load data=========================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense,Concatenate\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.models import Model\n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/one_word_spell_correction_noblog_dataset','rb') as f:\n",
        "    train_texts,train_output = pickle.load(f)\n",
        "\n",
        "# convert string to lower case\n",
        "# train_texts = train_df[1].values\n",
        "\n",
        "train_texts = [s.lower() for s in train_texts]\n",
        "for i in range(len(train_texts)):\n",
        "#   print(i)\n",
        "  train_texts[i] = train_texts[i].replace(' ','')\n",
        "\n",
        "# test_texts = test_df[1].values\n",
        "# test_texts = [s.lower() for s in test_texts]\n",
        "\n",
        "# =======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "# tk.fit_on_texts(train_texts)\n",
        "# If we already have a character list, then replace the tk.word_index\n",
        "# If not, just skip below part\n",
        "\n",
        "# -----------------------Skip part start--------------------------\n",
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789 \"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "# -----------------------Skip part end----------------------------\n",
        "\n",
        "# Convert string to index\n",
        "train_sequences = tk.texts_to_sequences(train_texts)\n",
        "# my_texts = list(map(sequence_to_text, sentences))\n",
        "\n",
        "# my_texts = sequence_to_text(train_sequences[:2])\n",
        "# print(type(train_sequences))\n",
        "# test_texts = tk.texts_to_sequences(test_texts)\n",
        "\n",
        "# Creating texts \n",
        "# Padding\n",
        "train_data = pad_sequences(train_sequences, maxlen=13, padding='post')\n",
        "# print(type(train_data))\n",
        "# test_data = pad_sequences(test_texts, maxlen=50, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "train_data = np.array(train_data, dtype='float32')\n",
        "# print(my_texts)\n",
        "# train_output = np.array(train_output, dtype='float32')\n",
        "\n",
        "# =======================Get classes================\n",
        "# train_classes = train_df[0].values\n",
        "# train_class_list = [x - 1 for x in train_output]\n",
        "# train_class_list = train_output\n",
        "# test_classes = test_df[0].values\n",
        "# test_class_list = [x - 1 for x in test_classes]\n",
        "\n",
        "# from keras.utils import to_categorical\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(train_output)\n",
        "# inverted = label_encoder.inverse_transform([train_output[0]])\n",
        "print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)\n",
        "# invert first example\n",
        "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[-1, :])])\n",
        "print(inverted)\n",
        "# enc = OneHotEncoder()#handle_unknown='ignore')\n",
        "# enc.fit(train_output)\n",
        "# train_classes = to_categorical(train_output)\n",
        "# test_classes = to_categorical(test_class_list)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AD-cznLzgTB1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# =====================Char CNN=======================\n",
        "# parameter\n",
        "from keras.layers import Concatenate,Add,BatchNormalization,Dropout,Bidirectional,LSTM\n",
        "from keras.utils import plot_model\n",
        "\n",
        "input_size = 13\n",
        "vocab_size = len(tk.word_index)\n",
        "embedding_size = len(tk.word_index)\n",
        "conv_layers = [[256, 10],\n",
        "               [256, 10],\n",
        "               [256, 7],\n",
        "               [256, 7],\n",
        "               [256, 7],\n",
        "               [256, 3],\n",
        "               [256, 3],]\n",
        "fully_connected_layers = [128, 128]\n",
        "num_of_classes = len(np.unique(integer_encoded))\n",
        "dropout_p = 0.5\n",
        "optimizer = 'adam'\n",
        "loss = 'categorical_crossentropy'\n",
        "\n",
        "# Embedding weights\n",
        "embedding_weights = []  # (70, 69)\n",
        "embedding_weights.append(np.zeros(vocab_size))  # (0, 69)\n",
        "\n",
        "for char, i in tk.word_index.items():  # from index 1 to 69\n",
        "    onehot = np.zeros(vocab_size)\n",
        "    onehot[i - 1] = 1\n",
        "    embedding_weights.append(onehot)\n",
        "\n",
        "embedding_weights = np.array(embedding_weights)\n",
        "print('Load')\n",
        "\n",
        "# Embedding layer Initialization\n",
        "embedding_layer = Embedding(vocab_size + 1,\n",
        "                            embedding_size,\n",
        "                            input_length=input_size,\n",
        "                            weights=[embedding_weights],\n",
        "                            trainable=False)\n",
        "\n",
        "# Model Construction\n",
        "# Input\n",
        "inputs = Input(shape=(input_size,), name='input', dtype='int64')  # shape=(?, 1014)\n",
        "# Embedding\n",
        "x = embedding_layer(inputs)\n",
        "# Conv\n",
        "# for filter_num, filter_size in conv_layers:\n",
        "#     if filter_size == 10:\n",
        "# #       if k == 0:\n",
        "#         x_10 = Conv1D(filter_num, filter_size)(x)\n",
        "#         x_10 = Activation('relu')(x_10)\n",
        "# #         k = 1\n",
        "# #       else:\n",
        "# #         x_10 = Conv1D(filter_num, filter_size)(x_10)\n",
        "# #         x_10 = Activation('relu')(x_10)\n",
        "#     elif filter_size == 7:\n",
        "# #       if d == 0:\n",
        "#         x_7 = Conv1D(filter_num, filter_size)(x)\n",
        "#         x_7 = Activation('relu')(x_7)\n",
        "# #         d = 1\n",
        "# #       else:\n",
        "# #         x_7 = Conv1D(filter_num, filter_size)(x_7)\n",
        "# #         x_7 = Activation('relu')(x_7)\n",
        "#     elif filter_size == 5:\n",
        "# #       if v == 0:\n",
        "#         x_5 = Conv1D(filter_num, filter_size)(x)\n",
        "#         x_5 = Activation('relu')(x_5)\n",
        "# #         v = 1\n",
        "# #       else:\n",
        "# #         x_5 = Conv1D(filter_num, filter_size)(x_5)\n",
        "# #         x_5 = Activation('relu')(x_5)\n",
        "#     elif filter_size == 3:\n",
        "# #       if b == 0:\n",
        "#         x_3 = Conv1D(filter_num, filter_size)(x)\n",
        "#         x_3 = Activation('relu')(x_3)\n",
        "# #         b = 1\n",
        "# #       else:\n",
        "# #         x_3 = Conv1D(filter_num, filter_size)(x)\n",
        "# #         x_3 = Activation('relu')(x_3)\n",
        "        \n",
        "# x = Add()([x_10,x_7,x_5,x_3])\n",
        "# conv_layers = [[256,10],[256,10],[256,7],[256,7],[256,7],[256,3],[256,3]]\n",
        "conv = Conv1D(256, 13,padding='same')(x)\n",
        "conv = BatchNormalization()(conv)\n",
        "conv = Activation('elu')(conv)\n",
        "conv = Dropout(0.3)(conv)\n",
        "conv = Conv1D(100, 13,padding='same')(conv)\n",
        "conv = BatchNormalization()(conv)\n",
        "conv = Activation('elu')(conv)\n",
        "conv = Dropout(0.3)(conv)\n",
        "\n",
        "x = Bidirectional(LSTM(25,return_sequences=True))(x)\n",
        "\n",
        "# for filter_num, filter_size in conv_layers:\n",
        "# #     if filter_size == 10:\n",
        "#       x = Conv1D(filter_num, filter_size,padding='same')(x)\n",
        "#       x = BatchNormalization()(x)\n",
        "#       x = Activation('relu')(x)\n",
        "#       x = Dropout(0.3)(x)\n",
        "#       x = Activation('elu')(x)\n",
        "\n",
        "#     if pooling_size != -1:\n",
        "#         x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)\n",
        "x = Concatenate()([conv,x])\n",
        "\n",
        "# bi = Flatten()(x)\n",
        "x = Flatten()(x)  # (None, 8704)\n",
        "# Fully connected layers\n",
        "# for dense_size in fully_connected_layers:\n",
        "#     x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024\n",
        "#     x = Dropout(dropout_p)(x)\n",
        "# Output Layer\n",
        "predictions = Dense(num_of_classes, activation='softmax')(x)\n",
        "# Build model\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, categorical_crossentropy\n",
        "model.summary()\n",
        "\n",
        "# # 1000 training samples and 100 testing samples\n",
        "# indices = np.arange(train_data.shape[0])\n",
        "# np.random.shuffle(indices)\n",
        "#\n",
        "# x_train = train_data[indices][:1000]\n",
        "# y_train = train_classes[indices][:1000]\n",
        "#\n",
        "# x_test = test_data[:100]\n",
        "# y_test = test_classes[:100]\n",
        "\n",
        "indices = np.arange(train_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "x_train = train_data[indices]\n",
        "y_train = train_classes[indices]\n",
        "\n",
        "plot_model(model, to_file='/content/drive/My Drive/singewordvisualmodel.png')\n",
        "\n",
        "# Training\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QrDKIT63gXSW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "          validation_split=0.01,\n",
        "          batch_size=32,\n",
        "          epochs=20,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ljGZ3WragafH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data(x):\n",
        "  # Convert string to index\n",
        "#   train_sequences = tk.texts_to_sequences(train_texts)\n",
        "  test_texts = tk.texts_to_sequences(x)\n",
        "\n",
        "  # Padding\n",
        "#   train_data = pad_sequences(train_sequences, maxlen=50, padding='post')\n",
        "  test_data = pad_sequences(test_texts, maxlen=13, padding='post')\n",
        "\n",
        "  # Convert to numpy array\n",
        "  x = np.array(test_data, dtype='float32')\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yk7amJtWgeTJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predicted = model.predict(data(['wedd']))\n",
        "# print(predicted)\n",
        "# print(predicted[-1,:])\n",
        "inverted = label_encoder.inverse_transform([argmax(predicted[-1, :])])\n",
        "print(inverted)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mwKzC8XighTQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    model_json = model.to_json()\n",
        "    with open(\"/content/drive/My Drive/final_spell_correction/cnn_bilstm_one_word_spell_correction.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "except:\n",
        "    pass\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"/content/drive/My Drive/final_spell_correction/cnn_bilstm_one_word_correction.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1oaWhItgjnj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# two word spell correction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zNiQmGDVgm6X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# write all code in one cell\n",
        "\n",
        "# ========================Load data=========================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense,Concatenate\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.models import Model\n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/two_word_spell_correction_noblog_dataset','rb') as f:\n",
        "    train_texts,train_output = pickle.load(f)\n",
        "train_output_1 = []\n",
        "for k,i in enumerate(train_output):\n",
        "    train_output_1.append(i[1])\n",
        "    del train_output[k][1]\n",
        "# convert string to lower case\n",
        "# train_texts = train_df[1].values\n",
        "# for l,i in enumerate(train_texts):\n",
        "#   if isinstance(i,list):\n",
        "#     train_texts[l] = i[0]\n",
        "# train_texts = [s.lower() for s in train_texts]\n",
        "# for i in range(len(train_texts)):\n",
        "# #   print(i)\n",
        "#   train_texts[i] = train_texts[i].replace(' ','')\n",
        "\n",
        "# test_texts = test_df[1].values\n",
        "# test_texts = [s.lower() for s in test_texts]\n",
        "\n",
        "# =======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(train_texts)\n",
        "# If we already have a character list, then replace the tk.word_index\n",
        "# If not, just skip below part\n",
        "\n",
        "# -----------------------Skip part start--------------------------\n",
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789 \"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "# -----------------------Skip part end----------------------------\n",
        "\n",
        "# Convert string to index\n",
        "train_sequences = tk.texts_to_sequences(train_texts)\n",
        "# test_texts = tk.texts_to_sequences(test_texts)\n",
        "\n",
        "# Padding\n",
        "train_data = pad_sequences(train_sequences, maxlen=13, padding='post')\n",
        "# test_data = pad_sequences(test_texts, maxlen=50, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "train_data = np.array(train_data, dtype='float32')\n",
        "# train_output = np.array(train_output, dtype='float32')\n",
        "\n",
        "# =======================Get classes================\n",
        "# train_classes = train_df[0].values\n",
        "# train_class_list = [x - 1 for x in train_output]\n",
        "# train_class_list = train_output\n",
        "# test_classes = test_df[0].values\n",
        "# test_class_list = [x - 1 for x in test_classes]\n",
        "\n",
        "# from keras.utils import to_categorical\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(train_output)\n",
        "print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)\n",
        "# invert first example\n",
        "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[-1, :])])\n",
        "print(inverted)\n",
        "\n",
        "label_encoder_1 = LabelEncoder()\n",
        "integer_encoded_1 = label_encoder_1.fit_transform(train_output_1)\n",
        "print(integer_encoded_1)\n",
        "# binary encode\n",
        "onehot_encoder_1 = OneHotEncoder(sparse=False)\n",
        "integer_encoded_1 = integer_encoded_1.reshape(len(integer_encoded_1), 1)\n",
        "onehot_encoded_1 = onehot_encoder_1.fit_transform(integer_encoded_1)\n",
        "print(onehot_encoded_1)\n",
        "# invert first example\n",
        "inverted_1 = label_encoder_1.inverse_transform([argmax(onehot_encoded_1[-1, :])])\n",
        "print(inverted_1)\n",
        "\n",
        "# label_encoder_2 = LabelEncoder()\n",
        "# integer_encoded_2 = label_encoder_2.fit_transform(train_output_2)\n",
        "# print(integer_encoded_2)\n",
        "# # binary encode\n",
        "# onehot_encoder_2 = OneHotEncoder(sparse=False)\n",
        "# integer_encoded_2 = integer_encoded_2.reshape(len(integer_encoded_2), 1)\n",
        "# onehot_encoded_2 = onehot_encoder_2.fit_transform(integer_encoded_2)\n",
        "# print(onehot_encoded_2)\n",
        "# # invert first example\n",
        "# inverted_2 = label_encoder_2.inverse_transform([argmax(onehot_encoded_2[-1, :])])\n",
        "# print(inverted_2)\n",
        "\n",
        "\n",
        "# enc = OneHotEncoder()#handle_unknown='ignore')\n",
        "# enc.fit(train_output)\n",
        "# train_classes = to_categorical(train_output)\n",
        "# test_classes = to_categorical(test_class_list)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPmolW0JgsnD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_classes = onehot_encoded\n",
        "train_classes_1 = onehot_encoded_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "41o0fgYagv7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# =====================Char CNN=======================\n",
        "# parameter\n",
        "from keras.layers import Concatenate,Add,BatchNormalization,Dropout,Bidirectional,LSTM\n",
        "from keras.utils import plot_model\n",
        "\n",
        "input_size = 13\n",
        "vocab_size = len(tk.word_index)\n",
        "embedding_size = len(tk.word_index)\n",
        "conv_layers = [[256, 10],\n",
        "               [256, 10],\n",
        "               [256, 7],\n",
        "               [256, 7],\n",
        "               [256, 7],\n",
        "               [256, 3],\n",
        "               [256, 3],]\n",
        "fully_connected_layers = [128, 128]\n",
        "num_of_classes = len(np.unique(integer_encoded))\n",
        "dropout_p = 0.5\n",
        "optimizer = 'adam'\n",
        "loss = 'categorical_crossentropy'\n",
        "\n",
        "# Embedding weights\n",
        "embedding_weights = []  # (70, 69)\n",
        "embedding_weights.append(np.zeros(vocab_size))  # (0, 69)\n",
        "\n",
        "for char, i in tk.word_index.items():  # from index 1 to 69\n",
        "    onehot = np.zeros(vocab_size)\n",
        "    onehot[i - 1] = 1\n",
        "    embedding_weights.append(onehot)\n",
        "\n",
        "embedding_weights = np.array(embedding_weights)\n",
        "print('Load')\n",
        "\n",
        "# Embedding layer Initialization\n",
        "embedding_layer = Embedding(vocab_size + 1,\n",
        "                            embedding_size,\n",
        "                            input_length=input_size,\n",
        "                            weights=[embedding_weights],\n",
        "                            trainable=False)\n",
        "\n",
        "# Model Construction\n",
        "# Input\n",
        "inputs = Input(shape=(input_size,), name='input', dtype='int64')  # shape=(?, 1014)\n",
        "# Embedding\n",
        "x = embedding_layer(inputs)\n",
        "# Conv\n",
        "# for filter_num, filter_size in conv_layers:\n",
        "#     if filter_size == 10:\n",
        "# #       if k == 0:\n",
        "#         x_10 = Conv1D(filter_num, filter_size)(x)\n",
        "#         x_10 = Activation('relu')(x_10)\n",
        "# #         k = 1\n",
        "# #       else:\n",
        "# #         x_10 = Conv1D(filter_num, filter_size)(x_10)\n",
        "# #         x_10 = Activation('relu')(x_10)\n",
        "#     elif filter_size == 7:\n",
        "# #       if d == 0:\n",
        "#         x_7 = Conv1D(filter_num, filter_size)(x)\n",
        "#         x_7 = Activation('relu')(x_7)\n",
        "# #         d = 1\n",
        "# #       else:\n",
        "# #         x_7 = Conv1D(filter_num, filter_size)(x_7)\n",
        "# #         x_7 = Activation('relu')(x_7)\n",
        "#     elif filter_size == 5:\n",
        "# #       if v == 0:\n",
        "#         x_5 = Conv1D(filter_num, filter_size)(x)\n",
        "#         x_5 = Activation('relu')(x_5)\n",
        "# #         v = 1\n",
        "# #       else:\n",
        "# #         x_5 = Conv1D(filter_num, filter_size)(x_5)\n",
        "# #         x_5 = Activation('relu')(x_5)\n",
        "#     elif filter_size == 3:\n",
        "# #       if b == 0:\n",
        "#         x_3 = Conv1D(filter_num, filter_size)(x)\n",
        "#         x_3 = Activation('relu')(x_3)\n",
        "# #         b = 1\n",
        "# #       else:\n",
        "# #         x_3 = Conv1D(filter_num, filter_size)(x)\n",
        "# #         x_3 = Activation('relu')(x_3)\n",
        "        \n",
        "# x = Add()([x_10,x_7,x_5,x_3])\n",
        "# conv_layers = [[256,10],[256,10],[256,7],[256,7],[256,7],[256,3],[256,3]]\n",
        "conv = Conv1D(256, 13,padding='same')(x)\n",
        "conv = BatchNormalization()(conv)\n",
        "conv = Activation('elu')(conv)\n",
        "conv = Dropout(0.3)(conv)\n",
        "conv = Conv1D(100, 13,padding='same')(conv)\n",
        "conv = BatchNormalization()(conv)\n",
        "conv = Activation('elu')(conv)\n",
        "conv = Dropout(0.3)(conv)\n",
        "\n",
        "x = Bidirectional(LSTM(25,return_sequences=True))(x)\n",
        "\n",
        "# for filter_num, filter_size in conv_layers:\n",
        "# #     if filter_size == 10:\n",
        "#       x = Conv1D(filter_num, filter_size,padding='same')(x)\n",
        "#       x = BatchNormalization()(x)\n",
        "#       x = Activation('relu')(x)\n",
        "#       x = Dropout(0.3)(x)\n",
        "#       x = Activation('elu')(x)\n",
        "\n",
        "#     if pooling_size != -1:\n",
        "#         x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)\n",
        "x = Concatenate()([conv,x])\n",
        "\n",
        "# bi = Flatten()(x)\n",
        "x = Flatten()(x)  # (None, 8704)\n",
        "# Fully connected layers\n",
        "# for dense_size in fully_connected_layers:\n",
        "#     x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024\n",
        "#     x = Dropout(dropout_p)(x)\n",
        "# Output Layer\n",
        "predictions = Dense(num_of_classes, activation='softmax')(x)\n",
        "predictions_1 = Dense(num_of_classes+1, activation='softmax')(x)\n",
        "# Build model\n",
        "model = Model(inputs=inputs, outputs=[predictions,predictions_1])\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, categorical_crossentropy\n",
        "model.summary()\n",
        "\n",
        "# # 1000 training samples and 100 testing samples\n",
        "# indices = np.arange(train_data.shape[0])\n",
        "# np.random.shuffle(indices)\n",
        "#\n",
        "# x_train = train_data[indices][:1000]\n",
        "# y_train = train_classes[indices][:1000]\n",
        "#\n",
        "# x_test = test_data[:100]\n",
        "# y_test = test_classes[:100]\n",
        "\n",
        "indices = np.arange(train_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "x_train = train_data[indices]\n",
        "y_train = train_classes[indices]\n",
        "y_train_1 = train_classes_1[indices]\n",
        "\n",
        "# plot_model(model, to_file='/content/drive/My Drive/singewordvisualmodel.png')\n",
        "# \n",
        "# Training\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SUfqTh1Vg0lu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, [y_train,y_train_1],\n",
        "          validation_split=0.01,\n",
        "          batch_size=32,\n",
        "          epochs=10,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B_ASmNf2g41P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data(x):\n",
        "  # Convert string to index\n",
        "#   train_sequences = tk.texts_to_sequences(train_texts)\n",
        "  test_texts = tk.texts_to_sequences(x)\n",
        "\n",
        "  # Padding\n",
        "#   train_data = pad_sequences(train_sequences, maxlen=50, padding='post')\n",
        "  test_data = pad_sequences(test_texts, maxlen=13, padding='post')\n",
        "\n",
        "  # Convert to numpy array\n",
        "  x = np.array(test_data, dtype='float32')\n",
        "  return x\n",
        "predicted,predicted_1 = model.predict(data(['']))\n",
        "# print(predicted)\n",
        "# print(predicted[-1,:])\n",
        "inverted = label_encoder.inverse_transform([argmax(predicted[-1, :])])\n",
        "inverted_1 = label_encoder_1.inverse_transform([argmax(predicted_1[-1, :])])\n",
        "# inverted_2 = label_encoder_2.inverse_transform([argmax(predicted_2[-1, :])])\n",
        "\n",
        "\n",
        "print(inverted,inverted_1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pvxRnXHug8NN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    model_json = model.to_json()\n",
        "    with open(\"/content/drive/My Drive/final_spell_correction/cnn_bilstm_two_word_classification.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "except:\n",
        "    pass\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"/content/drive/My Drive/final_spell_correction/cnn_bilstm_two_word_classification_classification.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}